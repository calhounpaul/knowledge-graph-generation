FROM nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04
#FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04

ARG DEBIAN_FRONTEND=noninteractive

#SHELL ["sh", "-lc"]

RUN apt update
RUN apt install -y git python3 python3-pip

RUN python3 -m pip install --no-cache-dir --upgrade pip

RUN python3 -m pip install --no-cache-dir llama-index llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface llama-index-llms-huggingface

ARG REF=main

#RUN curl -fsSL https://ollama.com/install.sh | sh

WORKDIR /root

#sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama
#sudo chmod +x /usr/bin/ollama

RUN apt install -y curl

#RUN curl -L https://ollama.com/download/ollama-linux-amd64 -o ./ollama
RUN curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama
RUN apt install -y tmux wget
RUN python3 -m pip install --no-cache-dir wget llama-index-embeddings-ollama

RUN chmod +x /usr/bin/ollama